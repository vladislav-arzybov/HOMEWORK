### Создание Kubernetes кластера

На этом этапе необходимо создать [Kubernetes](https://kubernetes.io/ru/docs/concepts/overview/what-is-kubernetes/) кластер на базе предварительно созданной инфраструктуры.   Требуется обеспечить доступ к ресурсам из Интернета.

Это можно сделать двумя способами:

1. Рекомендуемый вариант: самостоятельная установка Kubernetes кластера.  
   а. При помощи Terraform подготовить как минимум 3 виртуальных машины Compute Cloud для создания Kubernetes-кластера. Тип виртуальной машины следует выбрать самостоятельно с учётом требовании к производительности и стоимости. Если в дальнейшем поймете, что необходимо сменить тип инстанса, используйте Terraform для внесения изменений.
   б. Подготовить [ansible](https://www.ansible.com/) конфигурации, можно воспользоваться, например [Kubespray](https://wiki.blacksoft.pro/ru/DevOps/Kubernetes/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-kubespray)  
   в. Задеплоить Kubernetes на подготовленные ранее инстансы, в случае нехватки каких-либо ресурсов вы всегда можете создать их при помощи Terraform.
2. Альтернативный вариант: воспользуйтесь сервисом [Yandex Managed Service for Kubernetes](https://cloud.yandex.ru/services/managed-kubernetes)  
  а. С помощью terraform resource для [kubernetes](https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/kubernetes_cluster) создать **региональный** мастер kubernetes с размещением нод в разных 3 подсетях      
  б. С помощью terraform resource для [kubernetes node group](https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/kubernetes_node_group)
  
Ожидаемый результат:

1. Работоспособный Kubernetes кластер.
2. В файле `~/.kube/config` находятся данные для доступа к кластеру.
3. Команда `kubectl get pods --all-namespaces` отрабатывает без ошибок.


### РЕШЕНИЕ:

[Terraform]()

> Через Terraform подготовим 1 master ноду и 2 worker ноды. Для создания я использовал цикл for_each, т.к. он позволяет гибко задать имена и ресурсы ВМ.

```
Apply complete! Resources: 8 added, 0 changed, 0 destroyed.

Outputs:

masters = {
  "master-node-1" = {
    "external_ip" = "158.160.41.143"
    "internal_ip" = "10.0.1.21"
  }
}
workers = {
  "worker-node-1" = {
    "external_ip" = "89.169.156.91"
    "internal_ip" = "10.0.1.8"
  }
  "worker-node-2" = {
    "external_ip" = "84.201.173.91"
    "internal_ip" = "10.0.1.5"
  }
}
```

<img width="1324" height="190" alt="изображение" src="https://github.com/user-attachments/assets/d0a47824-60ba-4543-bd77-d0356bf0b6e7" />

> Для удобства отлатки и тестирования заранее подготовил cloud-config с админской уз, mc и редактором nano.

```
#cloud-config
users:
  - name: ${username}
    groups: sudo
    shell: /bin/bash
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    ssh_authorized_keys:
      - ${ssh_public_key}
package_update: true
package_upgrade: false
packages:
  - nano
  - mc
```

> После создания ВМ информация по созданным ресурсам экспортируется в inventory.ini через local_file. В основе шаблона inventory.tftpl используется inventory [Kubespray](https://wiki.blacksoft.pro/ru/DevOps/Kubernetes/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-kubespray).

```
resource "local_file" "inventory_kubespray" {
  content = templatefile("${path.module}/inventory.tftpl", {
    masters = {
      for name, vm in yandex_compute_instance.k8s :
      name => {
        external_ip = vm.network_interface[0].nat_ip_address
        internal_ip = vm.network_interface[0].ip_address
      }
      if can(regex("master", name))
    }
    workers = {
      for name, vm in yandex_compute_instance.k8s :
      name => {
        external_ip = vm.network_interface[0].nat_ip_address
        internal_ip = vm.network_interface[0].ip_address
      }
      if can(regex("worker", name))
    }
  })
  filename = "ansible/inventory/inventory.ini"
}
```

<img width="560" height="406" alt="изображение" src="https://github.com/user-attachments/assets/5c4942c3-7bae-40d4-915d-f547f49f7db0" />

[Ansible]()

> Изначально планировалось создать единый плейбук для первичной настройки мастер ноды, запуска плейбука Kubespray и последующего копирования kubeconfig. Но запуск ansible внутри ansible это не лучшая практика, к тому же будет не видно ошибок которые могут возникнуть в процессе настройки кластера. Поэтому вместо запуска скрипта Kubespray в site.yaml (плейбук настройки мастер ноды) был добавлен модуль ansible.builtin.pause, который позволяет приостановить выполнение плейбука до завершения процесса установки кластера, после чего запускается процесса копирования kubeconfig. 

```
        - name: Пауза для ручного запуска Kubespray
          ansible.builtin.pause:
            prompt: |
              Первичная настройка завершена. Далее необходимо:
              1) Подключиться к master-node-1 вручную выполнив команду: ssh {{ user }}@{{ hostvars[inventory_hostname].ansible_host }}
              2) Перейти в каталог kubespray: cd kubespray 
              2) Запустить команду для настройки k8s кластера: ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v
              
              - Когда установка кластера на сервере master-node-1 будет завершена нажми ENTER для завершения настройки.

              - Если необходимо отменить дальнейшую установку нажмите Ctrl+c, затем "a".
```

> Таким образом, при запуске: ```ansible-playbook -i ansible/inventory/inventory.ini ansible/site.yaml``` выполняется ряд задач по первичной настройке сервера:
- установки необходимых пакетов
- копирования kubespray из github репозитория
- установки зависимостей из requirements.txt
- копирования ранее созданного inventory.ini
- прокидования ключей для доступа к worker-нодам
- добавление внешнего IP-адреса в TLS-сертификат API-сервера Kubernetes (необходимо для доступа к кластеру с локального ПК, в противном случае получим ошибку)


> Запуск с локального хоста на мастер ноде, прокидывание приватного ключа:
- ansible-playbook -i ansible/inventory/inventory.ini ansible/site.yaml

> Обновить TLS-сертификат Kubernetes API-сервера и включить в него внешний IP-адрес моего хоста
- Правим: inventory/<your_cluster>/group_vars/k8s_cluster/k8s-cluster.yml, строку:

```
supplementary_addresses_in_ssl_keys:
  - 89.169.135.188
```

- ansible-playbook -i inventory/mycluster/inventory.ini   --become --become-user=root   cluster.yml   -t "k8s-certificates"

- Повторно копируем конфиг и выдаем права:

```bash
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> Копирование конфига k8s на локальный хост
- scp reivol@89.169.135.188:/home/reivol/.kube/config /home/reivol/.kube/
- В конфиге /home/reivol/.kube/config, правим: server: https://127.0.0.1:6443, на server: https://89.169.135.188:6443 (внещний ip master-node)
- Проверяем: kubectl get pods --all-namespaces

> Правка файлов при первом запуске: supplementary_addresses_in_ssl_keys: [10.0.0.1, 10.0.0.2, 10.0.0.3, 46.21.245.44, 109.196.195.164]
- /home/reivol/kubespray/inventory/mycluster/group_vars/all/all.yml
- /home/reivol/kubespray/inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

> Узнать внешний ip из консоли:
- curl 2ip.ru


На мастер ноде установить:
- python3-pip
- ansible


#####################################
> На мастер ноде, автоматизировать в ансибл
- git clone https://github.com/kubernetes-sigs/kubespray.git
- sudo apt update
- sudo apt install python3-pip
- sudo apt install ansible
- cd kubespray
- pip install -r requirements.txt

> Копируем шаблон в новый каталог
- cp -rfp inventory/sample/ inventory/mycluster

> Правим инвентари, копируем созданный!
- nano inventory/mycluster/inventory.ini

> Правим адрес мастер ноды!: supplementary_addresses_in_ssl_keys: [178.155.18.16]
- /home/reivol/kubespray/inventory/mycluster/group_vars/all/all.yml
- /home/reivol/kubespray/inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml

> Запускаем установку
- ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v

> На мастер ноде
- mkdir -p $HOME/.kube
- sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
- sudo chown $(id -u):$(id -g) $HOME/.kube/config

- kubectl get nodes
- kubectl get pods --all-namespaces

> На лок хосте: адрес мастер ноды
- scp reivol@89.169.155.95:/home/reivol/.kube/config /home/reivol/.kube/
- nano /home/reivol/.kube/config
- kubectl get pods --all-namespaces


<img width="840" height="370" alt="изображение" src="https://github.com/user-attachments/assets/5e644d47-8059-4f98-af26-3d7c9d69f404" />

