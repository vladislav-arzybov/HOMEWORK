### Создание Kubernetes кластера

На этом этапе необходимо создать [Kubernetes](https://kubernetes.io/ru/docs/concepts/overview/what-is-kubernetes/) кластер на базе предварительно созданной инфраструктуры.   Требуется обеспечить доступ к ресурсам из Интернета.

Это можно сделать двумя способами:

1. Рекомендуемый вариант: самостоятельная установка Kubernetes кластера.  
   а. При помощи Terraform подготовить как минимум 3 виртуальных машины Compute Cloud для создания Kubernetes-кластера. Тип виртуальной машины следует выбрать самостоятельно с учётом требовании к производительности и стоимости. Если в дальнейшем поймете, что необходимо сменить тип инстанса, используйте Terraform для внесения изменений.
   б. Подготовить [ansible](https://www.ansible.com/) конфигурации, можно воспользоваться, например [Kubespray](https://wiki.blacksoft.pro/ru/DevOps/Kubernetes/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-kubespray)  
   в. Задеплоить Kubernetes на подготовленные ранее инстансы, в случае нехватки каких-либо ресурсов вы всегда можете создать их при помощи Terraform.
2. Альтернативный вариант: воспользуйтесь сервисом [Yandex Managed Service for Kubernetes](https://cloud.yandex.ru/services/managed-kubernetes)  
  а. С помощью terraform resource для [kubernetes](https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/kubernetes_cluster) создать **региональный** мастер kubernetes с размещением нод в разных 3 подсетях      
  б. С помощью terraform resource для [kubernetes node group](https://registry.terraform.io/providers/yandex-cloud/yandex/latest/docs/resources/kubernetes_node_group)
  
Ожидаемый результат:

1. Работоспособный Kubernetes кластер.
2. В файле `~/.kube/config` находятся данные для доступа к кластеру.
3. Команда `kubectl get pods --all-namespaces` отрабатывает без ошибок.


### РЕШЕНИЕ:

[Terraform](https://github.com/vladislav-arzybov/HOMEWORK/tree/main/23_Diplom/02_k8s/02_infra)

> Через Terraform подготовим 1 master ноду и 2 worker ноды. Для создания я использовал цикл for_each, т.к. он позволяет гибко задать имена и ресурсы ВМ.

```
Outputs:

masters = {
  "master-node-1" = {
    "external_ip" = "158.160.41.143"
    "internal_ip" = "10.0.1.21"
  }
}
workers = {
  "worker-node-1" = {
    "external_ip" = "89.169.156.91"
    "internal_ip" = "10.0.1.8"
  }
  "worker-node-2" = {
    "external_ip" = "84.201.173.91"
    "internal_ip" = "10.0.1.5"
  }
}
```

<img width="1326" height="186" alt="изображение" src="https://github.com/user-attachments/assets/967a85d8-aff2-4738-96f9-bb4f460fb411" />

> Для удобства отладки и тестирования при создании ВМ через cloud-config прокидываются метаданные (уз, mc и редактор nano).

```
#cloud-config
users:
  - name: ${username}
    groups: sudo
    shell: /bin/bash
    sudo: ["ALL=(ALL) NOPASSWD:ALL"]
    ssh_authorized_keys:
      - ${ssh_public_key}
package_update: true
package_upgrade: false
packages:
  - nano
  - mc
```

> После создания ВМ информация по созданным ресурсам экспортируется в inventory.ini через local_file. В основе шаблона inventory.tftpl используется inventory [Kubespray](https://wiki.blacksoft.pro/ru/DevOps/Kubernetes/%D0%A1%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5-%D0%BA%D0%BB%D0%B0%D1%81%D1%82%D0%B5%D1%80%D0%B0-kubespray).

```
resource "local_file" "inventory_kubespray" {
  content = templatefile("${path.module}/inventory.tftpl", {
    masters = {
      for name, vm in yandex_compute_instance.k8s :
      name => {
        external_ip = vm.network_interface[0].nat_ip_address
        internal_ip = vm.network_interface[0].ip_address
      }
      if can(regex("master", name))
    }
    workers = {
      for name, vm in yandex_compute_instance.k8s :
      name => {
        external_ip = vm.network_interface[0].nat_ip_address
        internal_ip = vm.network_interface[0].ip_address
      }
      if can(regex("worker", name))
    }
  })
  filename = "ansible/inventory/inventory.ini"
}
```

<img width="560" height="406" alt="изображение" src="https://github.com/user-attachments/assets/5c4942c3-7bae-40d4-915d-f547f49f7db0" />

[Ansible](https://github.com/vladislav-arzybov/HOMEWORK/tree/main/23_Diplom/02_k8s/02_infra/ansible)

> Изначально планировал создать единый плейбук для первичной настройки мастер ноды, запуска плейбука Kubespray и последующего копирования kubeconfig. Но запуск ansible внутри ansible это не лучшая практика, к тому же не будет видно ошибок которые могут возникнуть в процессе настройки кластера. Поэтому вместо запуска скрипта Kubespray в site.yaml (плейбук настройки мастер ноды) был добавлен модуль ansible.builtin.pause, который позволяет приостановить выполнение плейбука до завершения процесса настройки кластера. 

```
        - name: Пауза для ручного запуска Kubespray
          ansible.builtin.pause:
            prompt: |
              Первичная настройка завершена. Далее необходимо:
              1) Подключиться к master-node-1 вручную выполнив команду: ssh {{ user }}@{{ hostvars[inventory_hostname].ansible_host }}
              2) Перейти в каталог kubespray: cd kubespray 
              2) Запустить команду для настройки k8s кластера: ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v
              
              - Когда установка кластера на сервере master-node-1 будет завершена нажми ENTER для завершения настройки.

              - Если необходимо отменить дальнейшую установку нажмите Ctrl+c, затем "a".
```

> Также в group_vars добавлена переменная user, что позволяет запускать плейбук под любым пользователем ранее указанным в ```${username}```  cloud-config.

```
user: reivol
```

> Таким образом, при запуске: ```ansible-playbook -i ansible/inventory/inventory.ini ansible/site.yaml``` выполняется ряд задач по первичной настройке сервера:
- установка необходимых пакетов
- копирование kubespray из github репозитория
- установка зависимостей из requirements.txt
- копирование ранее созданного inventory.ini
- прокидывание ключей для доступа к worker-нодам
- добавление внешнего IP-адреса в TLS-сертификат API-сервера Kubernetes (необходимо для доступа к кластеру с локального ПК, в противном случае получим ошибку ```"Unhandled Error" err="couldn't get current server API group list: Get \"https://89.169.135.188:6443/api?timeout=32s\": tls: failed to verify certificate: x509: certificate is valid for 10.233.0.1, 10.0.1.14, 127.0.0.1, ::1, not 89.169.135.188"```)

```
reivol@Zabbix:~/Terraform2/05_diplom/02_network$ ansible-playbook -i ansible/inventory/inventory.ini ansible/site.yaml

PLAY [Автоматическая настройка master-node-1] ************************************************************************************************************************************

TASK [Gathering Facts] ***********************************************************************************************************************************************************
ok: [master-node-1]

TASK [Установка необходимых пакетов] *********************************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование kubespray из github репозитория] *******************************************************************************************************************************
changed: [master-node-1]

TASK [Установка зависимостей из requirements.txt] ********************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование содержимого папки inventory/sample в папку inventory/mycluster] ************************************************************************************************
changed: [master-node-1]

TASK [Копирование файла inventory.ini] *******************************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование на master-node-1 приватного ключа] *****************************************************************************************************************************
changed: [master-node-1]

TASK [Добавление внешнего IP-адреса в TLS-сертификат API-сервера Kubernetes] *****************************************************************************************************
changed: [master-node-1]
```

> После успешного выполнения первого этапа первичной настройки матстер ноды появится сообщение с инструкцией по дальнейшим действиям на удаленном сервере, запуску kubespray.

<img width="997" height="181" alt="изображение" src="https://github.com/user-attachments/assets/266b161c-3831-48cf-b86a-c04eea29b567" />

> Подключаемся на сервер и выполняем команды:
- cd kubespray
- ansible-playbook -i inventory/mycluster/inventory.ini cluster.yml -b -v

> Дожидаемся завершения процесса настройки

<img width="1066" height="517" alt="изображение" src="https://github.com/user-attachments/assets/e5d15ad8-73fa-4d73-a133-f985ca8a406a" />

> После настройки кластера k8s можно вернуться в окно site.yaml и продолжить дальнейщую настройку мастер ноды, в этом блоке будут выполнены задания по:
- созданию каталога .kube для текущего пользователя
- копированию kubeconfig из /etc/kubernetes/admin.conf 
- выдаче необходимых прав для работы с kubectl
- а также копированию kubeconfig на локальную машину с заменой ip адреса

```
TASK [Создание директории .kube на master-node-1] ********************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование kubeconfig из /etc/kubernetes/admin.conf] **********************************************************************************************************************
changed: [master-node-1]

TASK [Изменение владельца kubeconfig] ********************************************************************************************************************************************
changed: [master-node-1]

TASK [Замена адреса в kubeconfig на внешний IP] **********************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование kubeconfig на локальную машину] ********************************************************************************************************************************
changed: [master-node-1]

TASK [Копирование kubeconfig для внутреннего доступа] ****************************************************************************************************************************
changed: [master-node-1]

PLAY RECAP ***********************************************************************************************************************************************************************
master-node-1              : ok=15   changed=13   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
```

> Настройка через ansible выполнена успешно, проверяем работу кластера на master ноде:
- kubectl get nodes
- kubectl get pods --all-namespaces

<img width="887" height="455" alt="изображение" src="https://github.com/user-attachments/assets/b3e78e6d-388f-4bbd-b0e0-a985c3111c59" />

> Также можно проверить доступность кластера с локальной машины на которой запускался terraform и ansible. Видно что доступ есть и настройка выполнена успешно.

<img width="856" height="453" alt="изображение" src="https://github.com/user-attachments/assets/7cf9a984-8732-4fc9-a609-ecf1ce4567ae" />

> Ссылки на конфигурационные файлы Terraform и ansible для выполнения данного блока:

- [Terraform](https://github.com/vladislav-arzybov/HOMEWORK/tree/main/23_Diplom/02_k8s/02_infra)
- [Ansible](https://github.com/vladislav-arzybov/HOMEWORK/tree/main/23_Diplom/02_k8s/02_infra/ansible)
